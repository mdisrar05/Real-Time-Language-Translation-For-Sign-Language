Abstract:

The "Real-time Language Translation for Sign Language" project aims to
bridgethe communication gap between hearing-impaired individuals who use sign
language and those who communicate through spoken or written language. It
combines computer vision and natural language processing to provide real-time
translation of sign language gestures into spoken and written forms. Through
advanced image and video processing, computer vision captures and interprets sign
language gestures using deep learning models such as convolutional neural
networks, mapping them to linguistic symbols. Natural language processing
techniques then translate these gestures into spoken or written language, facilitated
by machine translation models. Data collection, annotation, and model training are
vital components of the project, with a comprehensive dataset used for training to
ensure low-latency translation. This technology has broad applications,
empowering hearing-impaired individuals to communicate with a wider audience
and promoting inclusivity in various fields such as education, healthcare, customer
service, and public services. In conclusion, the project's fusion of computer vision,
NLP, and assistive technology has transformative potential, enhancing the quality
of life for the hearing-impaired and fostering inclusivity in society.



Step 1: Open this project in VS code and click on new terminal.

Step 2: Type: pip install requirements.txt

Step 3: Open the file 'inference_classifier.py' and run the file.

Step 4: A window 'frame' appears. Click 's' key to add letters and once the letters are added press 'p' key to show the entire word.

Step 5: Press 'q' key to close the window.
